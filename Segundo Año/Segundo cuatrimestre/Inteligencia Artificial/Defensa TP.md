-- -
# 1. ¬øQu√© es el **aprendizaje supervisado**?

Es un tipo de entrenamiento de modelos donde el algoritmo aprende **a partir de datos etiquetados**.  
Esto significa que **cada entrada (input)** en el conjunto de datos **tiene una salida (output) conocida**, es decir, una ‚Äúrespuesta correcta‚Äù.

### Ejemplo:

Si quer√©s entrenar un modelo para detectar fraudes:

- Input: caracter√≠sticas de una transacci√≥n (monto, hora, ubicaci√≥n, etc.)
    
- Output: etiqueta que indica si fue **fraude (1)** o **no fraude (0)**
    

El modelo **aprende a predecir** esa salida a partir de los ejemplos anteriores.

### Objetivo:

Aprender una **funci√≥n que relacione entradas con salidas** para poder predecir nuevas instancias correctamente.

---

## ¬øQu√© es el **aprendizaje no supervisado**?

En este caso, el modelo **no recibe etiquetas**. Solo se le dan los datos sin decirle cu√°l es la salida correcta.

### Ejemplo:

Un dataset con miles de transacciones, pero **sin saber cu√°les son fraudulentas** y cu√°les no.

### Objetivo:

Descubrir **patrones ocultos o agrupamientos (clusters)** en los datos.

---

## Diferencias clave

|Caracter√≠stica|Aprendizaje Supervisado|Aprendizaje No Supervisado|
|---|---|---|
|**Etiquetas (output conocido)**|‚úÖ S√≠|‚ùå No|
|**Ejemplos t√≠picos**|Clasificaci√≥n, regresi√≥n|Clustering, reducci√≥n de dimensionalidad|
|**Ejemplo pr√°ctico**|Detectar fraudes, reconocer spam|Agrupar clientes seg√∫n su comportamiento|
|**Entrada de datos**|Input + Output|Solo Input|
|**Resultado esperado**|Modelo que predice salidas conocidas|Estructura interna de los datos|

---

## En tu TP:

Usaste **aprendizaje supervisado** porque el modelo fue entrenado con **transacciones ya etiquetadas como fraude o no fraude**, y se busc√≥ que **aprendiera a clasificar nuevas transacciones** correctamente.

-- -

# 2. ¬øQu√© es una Red Neuronal Profunda?

Las **Redes Neuronales Profundas (DNN, por _Deep Neural Networks_)** son una clase de modelos de aprendizaje supervisado inspirados en el funcionamiento del cerebro humano. Son una extensi√≥n de las redes neuronales artificiales tradicionales, pero con **m√°s capas ocultas**, lo que les permite aprender representaciones complejas y abstractas de los datos.

---

Una DNN es una red neuronal que tiene:

- Una **capa de entrada** (input layer)
    
- **Varias capas ocultas** (hidden layers) ‚Äî esto es lo que la hace ‚Äúprofunda‚Äù
    
- Una **capa de salida** (output layer)
    

Cuantas m√°s **capas ocultas** tenga la red, **m√°s profunda** es.

---

### Estructura b√°sica de una DNN

```
Entrada ‚Üí Capa oculta 1 ‚Üí Capa oculta 2 ‚Üí ... ‚Üí Capa oculta N ‚Üí Salida
```

Cada capa est√° compuesta por **neuronas artificiales**, que aplican una funci√≥n matem√°tica sobre los datos y las **pesan** en base al entrenamiento.

---

## ¬øQu√© hacen las DNN?

Las redes neuronales profundas **aprenden representaciones jer√°rquicas de los datos**. Por ejemplo, en im√°genes:

- Las primeras capas pueden detectar bordes
    
- Las intermedias pueden detectar formas
    
- Las √∫ltimas pueden detectar objetos complejos
    

En tu caso, aplicado a **detecci√≥n de fraudes**:

- Capas iniciales pueden identificar patrones simples (como montos altos)
    
- Capas intermedias pueden identificar combinaciones sospechosas (ubicaci√≥n + hora)
    
- Capas finales deciden si es fraude o no
    

---

## ¬øPor qu√© son poderosas?

Las DNN:

- Aprenden **caracter√≠sticas autom√°ticamente** (no necesitan que se las definan manualmente)
    
- Pueden capturar **relaciones no lineales muy complejas**
    
- Funcionan muy bien con **grandes vol√∫menes de datos**
    

---

## Elementos clave en una DNN

|Componente|Descripci√≥n|
|---|---|
|**Neuronas**|Cada una realiza un c√°lculo: suma ponderada + funci√≥n de activaci√≥n|
|**Pesos (weights)**|Valores que indican la importancia de cada entrada|
|**Funciones de activaci√≥n**|Como ReLU, Sigmoid, Tanh. Introducen no linealidad|
|**Backpropagation**|Algoritmo que ajusta los pesos durante el entrenamiento|
|**Funci√≥n de p√©rdida (loss)**|Mide qu√© tan mal predice la red, para corregir errores|
|**Optimizaci√≥n (e.g., Adam)**|Algoritmo que busca minimizar la funci√≥n de p√©rdida|

---

## En tu TP

Usaron una red neuronal profunda con capas como:

- **Input Layer:** recibe los features de la transacci√≥n
    
- **Hidden Layers:** extraen combinaciones complejas de patrones
    
- **Output Layer:** devuelve 1 (fraude) o 0 (no fraude)
    

Y aplicaron:

- **Funciones de activaci√≥n ReLU y Sigmoid**
    
- **Backpropagation**
    
- **Funci√≥n de p√©rdida Binary Cross Entropy**
    
- **Optimizador Adam**
    

---

# 3. Funciones de Activaci√≥n: **ReLU** y **Sigmoid**

Las **funciones de activaci√≥n** son funciones matem√°ticas que se aplican a la salida de cada neurona. Le permiten a la red **modelar relaciones no lineales**, lo cual es esencial para problemas complejos como la detecci√≥n de fraudes.

### ReLU (_Rectified Linear Unit_)

**F√≥rmula:**  
$$  
f(x) = \max(0, x)  
$$

**Qu√© hace:**

- Devuelve el mismo valor si es positivo
    
- Devuelve 0 si es negativo
    

**Ventajas:**

- Simple y eficiente computacionalmente
    
- Ayuda a evitar el problema del **desvanecimiento del gradiente**
    
- Funciona muy bien en capas ocultas
    

**En tu TP:**  
Usaron **ReLU en las capas ocultas** para extraer patrones no lineales complejos de los datos.

---

### Sigmoid

**F√≥rmula:**  
$$  
f(x) = \frac{1}{1 + e^{-x}}  
$$

**Qu√© hace:**

- Comprime la salida entre 0 y 1
    
- Interpretable como una **probabilidad**
    

**En tu TP:**  
Usaron **Sigmoid en la capa de salida**, ya que el objetivo es **clasificar**:

- 0 = transacci√≥n normal
    
- 1 = transacci√≥n fraudulenta
    

---

## 2. Backpropagation

Es el **algoritmo de entrenamiento** que permite a la red aprender.

### ¬øQu√© hace?

Ajusta los **pesos de las conexiones** entre neuronas, **minimizando el error** en la predicci√≥n.

### ¬øC√≥mo funciona?

1. **Forward pass:** se calcula la salida de la red con los pesos actuales
    
2. **Se mide el error** entre la salida obtenida y la salida esperada (ground truth)
    
3. **Se propaga el error hacia atr√°s** desde la salida hasta la entrada
    
4. Se calcula el **gradiente de la funci√≥n de p√©rdida** con respecto a cada peso
    
5. Se **actualizan los pesos** para minimizar ese error
    

### ¬øCon qu√© se combina?

Con un **optimizador**, como **Adam**, que decide cu√°nto y c√≥mo ajustar los pesos.

---

## üìâ 3. Funci√≥n de P√©rdida: **Binary Cross Entropy**

Es una funci√≥n que mide qu√© tan mal est√°n las predicciones de la red respecto a los valores reales.  
Se usa en **clasificaci√≥n binaria** (como fraude vs no fraude).

### F√≥rmula:

$$  
\text{Loss} = -[y \cdot \log(\hat{y}) + (1 - y) \cdot \log(1 - \hat{y})]  
$$

Donde:

- $y$: valor real (0 o 1)
    
- $\hat{y}$: probabilidad predicha por la red
    

### ¬øPor qu√© se usa?

- Penaliza mucho cuando la red est√° muy equivocada
    
- Da resultados en escala de 0 a 1, ideal para tareas de clasificaci√≥n
    

**En tu TP:**  
La red fue entrenada para minimizar este error entre las predicciones y la realidad.

---

## 4. Optimizador **Adam**

Es un **algoritmo de optimizaci√≥n** muy usado para entrenar redes neuronales.

### ¬øQu√© hace?

Durante el backpropagation, decide **cu√°nto deben ajustarse los pesos** para reducir la p√©rdida.

### ¬øPor qu√© es tan usado?

- Combina lo mejor de dos algoritmos: **Momentum** y **RMSProp**
    
- Ajusta autom√°ticamente la tasa de aprendizaje para cada peso
    
- Es eficiente y robusto, ideal para datasets grandes y ruidosos
    

**En tu TP:**  
Se us√≥ **Adam** para optimizar los pesos y garantizar que la red converja m√°s r√°pido.

---

## En resumen, en tu TP:

|Componente|Rol|
|---|---|
|**ReLU**|Funci√≥n de activaci√≥n en capas ocultas|
|**Sigmoid**|Activaci√≥n final para devolver una probabilidad de fraude|
|**Binary Cross Entropy**|Mide el error de predicci√≥n en clasificaci√≥n binaria|
|**Backpropagation**|Propaga el error y ajusta los pesos|
|**Adam**|Decide c√≥mo ajustar los pesos para minimizar el error|

---

## Explicaci√≥n de StandardScaler y Stratify

Tanto **StandardScaler** como la estrategia **Stratify** son t√©cnicas fundamentales utilizadas en la fase de **Preprocesamiento de Datos** antes de entrenar un modelo de Machine Learning, como la Red Neuronal Profunda (DNN) propuesta en tu TP.

## 1. StandardScaler (Escalamiento de Datos)

**StandardScaler** es una t√©cnica de normalizaci√≥n o estandarizaci√≥n de datos.

- **¬øQu√© es?** Es el m√≥dulo que realiza el **Escalamiento** en el _pipeline_ de datos. Transforma las caracter√≠sticas para que tengan una media (promedio) de 0 y una desviaci√≥n est√°ndar de 1.
    
- **Prop√≥sito:** Asegurar que todas las variables num√©ricas tengan la **misma magnitud**.
    
- **Justificaci√≥n para la DNN:** Es **vital** para que el algoritmo de optimizaci√≥n (**Descenso de Gradiente**, como Adam) funcione de manera eficiente y justa. Esto evita que las caracter√≠sticas con valores num√©ricos grandes (como el monto de la transacci√≥n) dominen el proceso de entrenamiento de la red, impidiendo que el modelo aprenda correctamente de todas las caracter√≠sticas.
    

---

## 2. Stratify (Estrategia de Divisi√≥n Estratificada)

**Stratify** es una estrategia utilizada durante la **Divisi√≥n Train/Test** (separaci√≥n de los datos en conjuntos de entrenamiento y prueba).

- **¬øQu√© es?** Es una opci√≥n que se aplica a la divisi√≥n de los datos.
    
- **Prop√≥sito:** Garantizar que la **proporci√≥n de clases** (Fraude vs. No Fraude) en los conjuntos de entrenamiento y prueba sea la **misma** que la proporci√≥n original del _dataset_ completo.
    
- **Justificaci√≥n para el TP de Fraude:** En la detecci√≥n de fraude, los datos est√°n altamente **desequilibrados** (hay muy pocos casos de fraude). Usar _Stratify_ es esencial para asegurar que el rendimiento del modelo (especialmente el **Recall**) se eval√∫e correctamente sobre una distribuci√≥n de clases que represente la realidad del problema, evitando sesgos.

-- -
# **Explicacion diagramas**

## 1. Diagrama de Flujo del Sistema DNN

![[Pasted image 20251021153833.png]]


El diagrama ilustra el _pipeline_ (flujo de trabajo) de preparaci√≥n de datos para el **Sistema de Detecci√≥n de Fraude (DNN)**, desde que se reciben las transacciones hasta que est√°n listas para el entrenamiento del modelo de Inteligencia Artificial.

|**Paso**|**Bloque del Diagrama**|**Significado y Prop√≥sito**|
|---|---|---|
|**Inicio**|**Transacciones Brutas**|Es la materia prima: el conjunto de registros hist√≥ricos de transacciones financieras tal como se reciben. Estos datos incluyen variables con diferentes escalas (ej: montos grandes y horas peque√±as), valores categ√≥ricos (ej: tipo de tarjeta) y el desequilibrio natural de clases (mucho "No Fraude", poco "Fraude").|
|**1**|**Preprocesamiento y Escalamiento**|Se aplica la limpieza de datos (eliminar valores nulos o inconsistencias) y el **Escalamiento** (usando _StandardScaler_). Este paso es crucial para que todas las variables num√©ricas tengan la misma magnitud. Esto es vital para que el algoritmo de optimizaci√≥n (Descenso de Gradiente, como Adam) funcione de manera eficiente y justa, sin que una caracter√≠stica domine a las dem√°s solo por tener un valor num√©rico grande.|
|**2**|**Datos Escalados (D1)**|Es el resultado del primer paso: las transacciones est√°n ahora listas para el modelo (limpias, con variables categ√≥ricas codificadas y con magnitudes uniformes).|
|**3**|**Divisi√≥n Train/Test**|Los datos escalados se dividen en dos subconjuntos principales: **Entrenamiento (Train)** y **Prueba (Test)**. Esta divisi√≥n debe hacerse con la estrategia _stratify_ (estratificada) para asegurar que el conjunto de entrenamiento y el de prueba tengan una proporci√≥n similar de casos de fraude/no fraude.|
|**4**|**Conclusi√≥n**|Representa la fase final del proceso de modelado, donde la arquitectura DNN (Red Neuronal Profunda) entrenar√° con los datos de **Entrenamiento** y ser√° evaluada con los datos de **Prueba** para determinar su rendimiento (usando m√©tricas como Loss, Accuracy y **Recall**).|

## 2. Arquitectura completa del sistema de ML

![[Pasted image 20251021154519.png]]

Este diagrama es una representaci√≥n de la **arquitectura completa de un sistema de Machine Learning (ML)** dise√±ado para la detecci√≥n de fraude, mostrando las tres fases principales por las que pasan los datos y el modelo: **Infraestructura de Datos, Pipeline de Entrenamiento, y Modelo en Producci√≥n (Inferencia)**.

### 1. Infraestructura de Datos (Azul)

Esta fase se centra en obtener, limpiar y preparar los datos de entrada.

- **Fuente de Datos (_make_classification_):** Representa la entrada inicial de las **Transacciones Brutas**, que son los registros sin procesar.
    
- **M√≥dulo de Preprocesamiento (_StandardScaler_):** Se encarga de la limpieza y la transformaci√≥n inicial de las transacciones. Utiliza el **StandardScaler** para **escalar las variables** y asegurar que todas tengan la misma magnitud.
    
- **Almac√©n de Caracter√≠sticas (_X_scaled, y_):** Es el resultado del preprocesamiento, donde las caracter√≠sticas (_X_) ya est√°n escaladas y listas, junto con la etiqueta real (_y_) (Fraude/No Fraude).
    

---

### 2. Pipeline de ML/Entrenamiento (Naranja)

Esta es la fase de construcci√≥n y ajuste del modelo.

- **M√≥dulo de Preprocesamiento (_StandardScaler_):** Este m√≥dulo asegura que el modelo se entrena con los mismos pasos de escalado usados en la preparaci√≥n de los datos originales, pero aplicados solo a los **Datos de Entrenamiento**.
    
- **M√≥dulo de Entrenamiento (_TensorFlow Keras_):** Aqu√≠ es donde se construye, compila y entrena la **Red Neuronal Profunda (DNN)**. Utiliza las librer√≠as _TensorFlow Keras_ para el proceso de ajuste de pesos (_Backpropagation_) y creaci√≥n del **Modelo Entrenado**.
    
- **Registro de Modelos (_modelSave_):** Almacena y versiona el **Modelo Entrenado** para su uso futuro, mantenimiento o para ser utilizado en producci√≥n.
    
- **API de Inferencia (Predicci√≥n en tiempo real):** Muestra c√≥mo el modelo entrenado es **Desplegado** y est√° listo para recibir peticiones para hacer predicciones en tiempo real.
    

---

### 3. Modelo en Producci√≥n (Inferencia) (Verde)

Esta fase representa el uso del modelo en un entorno operativo para tomar decisiones sobre nuevas transacciones.

- **Registro de Modelos (_modelSave_):** El modelo entrenado se recupera de este registro.
    
- **API de Inferencia (Predicci√≥n en):** Es el servicio activo que recibe **nuevas transacciones** y aplica el modelo desplegado para clasificarlas inmediatamente como Fraude o No Fraude (la predicci√≥n en tiempo real).
-- -
## 3. Flujo de Implementaci√≥n de la DNN (Modelo de Detecci√≥n de Fraude)

![[Pasted image 20251021155829.png]]

Este diagrama ilustra los pasos secuenciales para construir, entrenar y validar el modelo de Machine Learning utilizando las librer√≠as Scikit-learn y TensorFlow/Keras.

| **Bloque del Diagrama**             | **Paso**                    | **Explicaci√≥n Funcional**                                                                                                                                                                                                                     |
| ----------------------------------- | --------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Simular/Cargar Datos                | **Carga de Datos (X, y)**   | Se simulan o cargan las transacciones (X, caracter√≠sticas) y sus etiquetas reales (y, Fraude/No Fraude).                                                                                                                                      |
| Instanciar y Aplicar StandardScaler | **Preprocesamiento**        | Se aplica el **StandardScaler** para **escalar las caracter√≠sticas** (X) y asegurar que todas las variables num√©ricas tengan la misma magnitud.                                                                                               |
| Dividir Datos                       | **Separaci√≥n Train/Test**   | Los datos se dividen en conjuntos de **entrenamiento** y **prueba** (80%/20%) utilizando la estrategia **stratify** para mantener el equilibrio de clases (Fraude/No Fraude) en ambos conjuntos.                                              |
| Definir Arquitectura DNN            | **Construcci√≥n del Modelo** | Se define la arquitectura secuencial de la Red Neuronal Profunda (DNN) con sus capas:                                                                                                                                                         |
| Capa Densa (64, ReLU, input)        | _1¬™ Capa Oculta_            | 64 neuronas con activaci√≥n **ReLU** (introduce no linealidad). Define la forma de la entrada (input).                                                                                                                                         |
| Capa Densa (32, RELU)               | _2¬™ Capa Oculta_            | 32 neuronas con activaci√≥n **ReLU**. Contin√∫a el procesamiento no lineal.                                                                                                                                                                     |
| Capa Densa (1, Sigmoid)             | _Capa de Salida_            | 1 neurona con activaci√≥n **Sigmoid**. Mapea la salida a una probabilidad entre 0 y 1 para la clasificaci√≥n binaria.                                                                                                                           |
| Compilar Modelo                     | **Configuraci√≥n**           | Se configura el modelo con el optimizador **Adam** y la funci√≥n de p√©rdida **Binary Crossentropy** (necesaria para problemas binarios).                                                                                                       |
| Entrenar Modelo                     | **Ajuste de Pesos**         | Se entrena el modelo (_model.fit_) con los datos de entrenamiento a lo largo de 10 √©pocas (_epochs_) y reservando una porci√≥n (_validation_split_) para la validaci√≥n interna del modelo. Este paso ejecuta el algoritmo **Backpropagation**. |
| Evaluar Modelo                      | **Prueba (Test)**           | Se eval√∫a el modelo entrenado (_model.evaluate_) utilizando el conjunto de datos de prueba (x_test, y_test) que el modelo nunca ha visto.                                                                                                     |
| Imprimir M√©tricas                   | **Reporte**                 | Se muestran las m√©tricas de rendimiento clave: **Loss** (p√©rdida final), **Accuracy** (precisi√≥n general), y **Recall** (sensibilidad, crucial para detectar fraudes).                                                                        |
| Guardar Modelo                      | **Despliegue**              | El modelo entrenado se guarda (_model.save_) para poder ser desplegado en producci√≥n y utilizarse para la inferencia en tiempo real.                                                                                                          |
| Fin                                 |                             | El proceso ha finalizado.                                                                                                                                                                                                                     |
--- -
## 4. Diagrama de Clases: Componentes de la DNN y Pipeline de Datos

![[Pasted image 20251021161721.png]]

Este diagrama es un **Diagrama de Clases** que representa la estructura de los componentes de software (clases y sus relaciones) utilizados para implementar el modelo de detecci√≥n de fraude con la Red Neuronal Profunda (DNN). Se enfoca en c√≥mo los datos se preparan y se utilizan para construir el modelo en Keras.

El diagrama muestra las clases principales, sus atributos (datos) y m√©todos (funciones), y c√≥mo interact√∫an para llevar a cabo el entrenamiento.

### 1. Clases de Datos y Preprocesamiento

| **Clase**       | **Prop√≥sito y Contenido**                                                                                                                                                                                                                                                                                                                                       | **Relaciones Clave**                                                                                          |
| --------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------- |
| **DataSet**     | Representa la fuente de datos. Contiene el m√©todo `make_classification` para simular o cargar las transacciones. Sus atributos son los datos de entrada **X** (caracter√≠sticas) y la etiqueta **y** (fraude/no fraude), ambos como _ndarray_ (arreglos de datos).                                                                                               | **Alimenta** el modelo (`Preprocessor` de Keras) con los datos ya divididos y escalados.                      |
| **Preprocesor** | Representa las herramientas de preparaci√≥n de datos (Scikit-learn). Contiene el m√©todo `StandardScaler` para el escalamiento y el m√©todo `fit_transform(X)` para aplicar el escalamiento. Tambi√©n contiene el m√©todo `train_test_split(X_scaled, y)` para dividir los datos en conjuntos de entrenamiento (`x_train`, `y_train`) y prueba (`x_test`, `y_test`). | **Usa/Alimenta** a la clase `Preprocessor` (entrenamiento) y a la clase `Preprocessor` (preparaci√≥n inicial). |


### 2. Clase del Modelo (Keras)

| **Clase**                | **Prop√≥sito y Contenido**                                                                                                                                                                                                                                                                                                                                                                     | **Relaciones Clave**                                                                                                          |
| ------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------- |
| **Preprocessor** (Keras) | Representa el flujo de trabajo de Machine Learning y el modelo en s√≠ (construcci√≥n y entrenamiento) usando **TensorFlow Keras**. Contiene el constructor `Sequential(model)` y el m√©todo `add(Layer)` para a√±adir capas. Incluye los m√©todos `compile` (para definir el optimizador, la funci√≥n de p√©rdida y las m√©tricas) y `fit` (para ejecutar el entrenamiento a lo largo de las √©pocas). | **Contiene** instancias de la clase `DenseLayer` (las capas de la DNN). **Usa** la clase `DenseLayer` para evaluar el modelo. |
| **DenseLayer**           | Representa la unidad fundamental de la Red Neuronal Profunda. Define los par√°metros de cada capa de la DNN (64, 32 y 1 neurona). Sus atributos son: **units** (n√∫mero de neuronas), **activation** (ej: RELU, Sigmoid) y **input_shape** (la dimensi√≥n de la entrada).                                                                                                                        | Es **contenida** por el `Preprocessor` (Keras) para formar la arquitectura de la red (Capas 64/32/1).                         |

En esencia, el diagrama muestra un ciclo donde los datos son cargados, limpiados y divididos (clase `Preprocessor` de Scikit-learn), y luego usados para construir y ajustar el modelo de capas densas (`Preprocessor` de Keras) mediante el entrenamiento (`fit`)
-- -

## 5. Diagrama de Informaci√≥n del Sistema de Detecci√≥n de Fraude

![[Pasted image 20251021163152.png]]

Este diagrama modela la estructura de la base de datos y los flujos de informaci√≥n del sistema, mostrando c√≥mo los datos crudos se preparan, entrenan y registran para la inferencia del modelo DNN.

### 1. Entidades de Datos Crudos

| **Entidad**     | **Prop√≥sito y Atributos Clave**                                                                                        | **Relaci√≥n**                                                          |
| --------------- | ---------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------- |
| **TRANSACTION** | Contiene los registros generales de las transacciones. Incluye la etiqueta real de lo que se intenta predecir.         | **Tiene** una relaci√≥n con el `Preprocessor` (que utiliza sus datos). |
|                 | `transaction_id` (Clave Primaria), `float amount`, `int is_fraud` (Etiqueta Real).                                     |                                                                       |
| **FEATURE**     | Contiene las caracter√≠sticas de ingenier√≠a que se alimentan al modelo DNN. Cada registro se vincula a una transacci√≥n. | **Utiliza** el `Preprocessor` para escalar y dividir sus datos.       |
|                 | `feature_id` (Clave Primaria), `transaction_id` (Clave For√°nea), y las `float feature_v1` a `feature_v15`.             |                                                                       |

### 2. M√≥dulo de Preprocesamiento (Motor de Datos)

|**Clase**|**Prop√≥sito y M√©todos Clave**|**Relaciones Clave**|
|---|---|---|
|**Preprocessor**|Representa el c√≥digo que maneja la preparaci√≥n de datos (Scikit-learn/Keras). Sus m√©todos transforman los datos de las tablas `TRANSACTION` y `FEATURE`.|**Utiliza** datos de `TRANSACTION` y `FEATURE`. **Genera** la `MODEL_VERSION` (ya que sus m√©todos definen c√≥mo se usan los datos para entrenar el modelo).|
||`StandardScaler scaler`, `fit_transform(X)` (para escalamiento), `train_test_split(X_scaled, y)` (para dividir los datos).||

### 3. Entidades de Modelo y Predicci√≥n

|**Entidad**|**Prop√≥sito y Atributos Clave**|**Relaci√≥n**|
|---|---|---|
|**MODEL_VERSION**|Almacena y versiona el modelo DNN entrenado. Registra el algoritmo usado y el rendimiento obtenido.|Es **generada** por el `Preprocessor` al finalizar el entrenamiento. **Contiene** la tabla `FRAUD_PREDICTION` (almacena las predicciones hechas por esta versi√≥n).|
||`model_version_id` (Clave Primaria), `string algorithm` (DNN), `trained_date`, `recall_score` (m√©trica clave).||
|**FRAUD_PREDICTION**|Almacena los resultados de la inferencia (predicciones) hechas por una versi√≥n espec√≠fica del modelo.|Es **contenida** por la `MODEL_VERSION`.|
||`prediction_id` (Clave For√°nea), `model_version_id` (Clave For√°nea), `risk_probabilities` (la salida Sigmoid).||

El diagrama ilustra un flujo integral desde los **Datos Brutos** (Tablas `TRANSACTION`/`FEATURE`) $\to$ **Preparaci√≥n (`Preprocessor`)** $\to$ **Modelo Entrenado (`MODEL\_VERSION`)** $\to$ **Resultado/Inferencia (`FRAUD\_PREDICTION`)**.

--- -
## 6. Flujo del Ciclo de Vida del Modelo DNN

![[Pasted image 20251021163700.png]]


Este diagrama de flujo ilustra el **ciclo de vida del modelo de Red Neuronal Profunda (DNN)**, desde su definici√≥n hasta su despliegue, usando la estructura de la librer√≠a Keras (Python).

Es una vista de alto nivel del proceso de **Entrenamiento y Validaci√≥n** que se da en tu TP de detecci√≥n de fraude.

| **Bloque del Diagrama** | **Fase**                          | **Explicaci√≥n del Proceso**                                                                                                                                                                                                                                                                                                                               |
| ----------------------- | --------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Inicializado**        | **Definici√≥n del Flujo**          | Comienza con la llamada a `Sequential()`, que establece que la red neuronal tendr√° un flujo de datos lineal, donde la informaci√≥n pasa secuencialmente de una capa a la siguiente.                                                                                                                                                                        |
| **Construyendo**        | **Arquitectura (Topolog√≠a)**      | Se a√±aden las capas densas (`model.add(Dense)`). La arquitectura elegida es **64 ‚Üí 32 ‚Üí 1**, que consiste en dos capas ocultas (64 y 32 neuronas, ambas con activaci√≥n **ReLU** para introducir no linealidad) y una capa de salida (1 neurona con activaci√≥n **Sigmoid** para clasificaci√≥n binaria).                                                    |
| **Compilando**          | **Configuraci√≥n del Aprendizaje** | Se configura el modelo (`model.compile()`) antes del entrenamiento. Se define el: **Optimizer=Adam** (el algoritmo de Descenso de Gradiente), y **Loss=BinaryCrossentropy** (la funci√≥n de p√©rdida para medir el error en un problema binario).                                                                                                           |
| **Entrenando**          | **Ajuste de Pesos**               | Es la ejecuci√≥n de la funci√≥n `model.fit()`, donde se alimentan los datos de entrenamiento. En cada paso se aplica el algoritmo de **Backpropagation** para ajustar los pesos sin√°pticos de la red y minimizar la p√©rdida.                                                                                                                                |
| **Validando**           | **Monitoreo Interno**             | Durante el entrenamiento, se reserva una porci√≥n de los datos (`validation_split`) para hacer una **predicci√≥n interna**. Esto permite monitorear el rendimiento del modelo en datos no vistos en ese _epoch_, ayudando a detectar si est√° ocurriendo un sobreajuste. El proceso de _Entrenando_ y _Validando_ se repite hasta que se cumplen las √©pocas. |
| **Entrenado**           | **Guardar Resultado**             | Una vez que se cumplen todas las √©pocas de entrenamiento, se llama a `model.save()` para guardar el modelo final ajustado (sus pesos y arquitectura).                                                                                                                                                                                                     |
| **Desplegado**          | **Uso en Producci√≥n**             | El modelo guardado se pasa a la **API** (Application Programming Interface), lo que significa que est√° listo para la **Inferencia** (clasificar nuevas transacciones en tiempo real).                                                                                                                                                                     |
